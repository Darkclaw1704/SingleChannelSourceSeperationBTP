{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656ce275",
   "metadata": {},
   "source": [
    "TRANSFORMER FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0acba7d",
   "metadata": {},
   "source": [
    "The dpft_model.py file is an implementation of the transformer paper's blind source separation architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "FEATURE_DIM_N = 128\n",
    "N_HEADS = 2 \n",
    "KERNEL_SIZE = (4, 4)\n",
    "STRIDES = (2, 2)\n",
    "\n",
    "\n",
    "\n",
    "# N_FFT = 128  \n",
    "N_FFT=512\n",
    "# HOP_LENGTH = 128\n",
    "HOP_LENGTH = 256\n",
    "# ORIGINAL_LENGTH = 16384\n",
    "ORIGINAL_LENGTH = 65280\n",
    "\n",
    "\n",
    "DROPOUT_RATE = 0.1\n",
    "I_TRANSFORMER_LAYERS = 2\n",
    "J_DPTF_STACKS = 2\n",
    "\n",
    "def compute_stft_safe(x, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=None):\n",
    "    if win_length is None: win_length = n_fft\n",
    "    window = torch.hann_window(win_length, device=x.device)\n",
    "    \n",
    "    if x.shape[-1] < n_fft:\n",
    "        pad_amount = n_fft - x.shape[-1]\n",
    "        x = F.pad(x, (0, pad_amount))  \n",
    "    \n",
    "    Xm = torch.stft(\n",
    "        x, n_fft=n_fft, hop_length=hop_length, win_length=win_length,\n",
    "        window=window, return_complex=True\n",
    "    )\n",
    "    return Xm\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_signal(xm):\n",
    "    \"\"\"\n",
    "    xm: [B, C, T]  (batch, channels, time)\n",
    "    Returns:\n",
    "        Xm2_4d: [B, 4, F_padded, T_new] - LogAmp/Phase stack\n",
    "        Xm_original_batched: [B, F, T_new] - Complex STFT of the *first* channel\n",
    "    \"\"\"\n",
    "    B, C, T_old = xm.shape\n",
    "    Xm2_4d_list = []\n",
    "    Xm_original_batched_list = []\n",
    "\n",
    "\n",
    "\n",
    "    for b in range(B):\n",
    "        sig_mix = xm[b, 0] \n",
    "        Xm_mix = compute_stft_safe(sig_mix) \n",
    "        Xm_original_batched_list.append(Xm_mix)\n",
    "        \n",
    "        F, T_new = Xm_mix.shape\n",
    "        F_padded = F + 1\n",
    "        \n",
    "        channel_features = []\n",
    "        for c in range(C):\n",
    "            sig = xm[b, c] \n",
    "            Xm_c = compute_stft_safe(sig) \n",
    "            Xm_c = torch.cat([Xm_c, torch.zeros(1, T_new, device=Xm_c.device)], dim=0)\n",
    "            Xm_abs = torch.abs(Xm_c)\n",
    "            Xm_logamp = torch.log10(torch.clamp(Xm_abs, min=1e-10))\n",
    "            Xm_phase = torch.angle(Xm_c)\n",
    "            channel_features.append(Xm_logamp)\n",
    "            channel_features.append(Xm_phase)  \n",
    "        Xm2_4d_list.append(torch.stack(channel_features, dim=0))\n",
    "    Xm2_4d = torch.stack(Xm2_4d_list, dim=0)\n",
    "    Xm_original_batched = torch.stack(Xm_original_batched_list, dim=0)\n",
    "    return Xm2_4d, Xm_original_batched \n",
    "\n",
    "\n",
    "\n",
    "def post_processing_block(Xs_4d,Xm_original_batched):\n",
    "    Xs_logamp = Xs_4d[:, :2] \n",
    "    Xs_phase = Xs_4d[:, 2:]  \n",
    "    Xs_linamp = torch.pow(10, Xs_logamp)\n",
    "    M_real = Xs_linamp * torch.cos(Xs_phase)\n",
    "    M_imag = Xs_linamp * torch.sin(Xs_phase)\n",
    "    Xo_4d_complex = torch.complex(M_real, M_imag)\n",
    "    Xo_4d_complex = Xo_4d_complex[:, :, :-1, :]\n",
    "    Xo1 = Xo_4d_complex[:, 0] \n",
    "    Xo2 = Xo_4d_complex[:, 1] \n",
    "    Xo1_separated = Xo1 * Xm_original_batched\n",
    "    Xo2_separated = Xo2 * Xm_original_batched\n",
    "    \n",
    "    return Xo1_separated, Xo2_separated\n",
    "\n",
    "def compute_istft(X, length=ORIGINAL_LENGTH):\n",
    "    window = torch.hann_window(N_FFT, device=X.device)\n",
    "    xm_out = torch.istft(\n",
    "        X, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=N_FFT,\n",
    "        window=window, return_complex=False, length=length\n",
    "    )\n",
    "    return xm_out.squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "class PermuteLayerNorm(nn.Module):\n",
    "    \"\"\"Applies LayerNorm over the feature dimension N after permuting [B, C, F, T] -> [B, F, T, C]\"\"\"\n",
    "    def __init__(self, normalized_shape):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(normalized_shape)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2) \n",
    "        return x\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Simulates SeparableConv2D used in the paper (II.C)\n",
    "    The paper cites Keras/TensorFlow, which has a native separable layer.\n",
    "    In PyTorch, this is implemented as Depthwise followed by Pointwise convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels, bias=False\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, N=FEATURE_DIM_N, K=KERNEL_SIZE, S=STRIDES):\n",
    "        super().__init__()\n",
    "      \n",
    "\n",
    "        self.separable_conv1 = SeparableConv2d(2, N, kernel_size=K, stride=1, padding=(K[0]//2, K[1]//2))\n",
    "\n",
    "        self.norm1 = PermuteLayerNorm(N)\n",
    "      \n",
    "        self.separable_conv_down = SeparableConv2d(N, N, kernel_size=K, stride=S)\n",
    "        self.norm_down = PermuteLayerNorm(N)\n",
    "      \n",
    "        self.conv_final = nn.Conv2d(N, N, kernel_size=1)\n",
    "        self.norm_final = PermuteLayerNorm(N)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.norm1(self.separable_conv1(x))\n",
    "\n",
    "        x = self.norm_down(self.separable_conv_down(x))\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        Xe = self.norm_final(self.conv_final(x))\n",
    "        return Xe\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal Positional Encoding as required by the paper (II.D, Eq. 8)\"\"\"\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, Z):\n",
    "        seq_len = Z.size(1)\n",
    "        E = self.pe[:seq_len, :Z.size(2)]\n",
    "        return Z + E.unsqueeze(0) \n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements I stacks of vanilla Transformer Encoder layers (Fig. 2c).\n",
    "    Applies g(.) I times.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, dropout=DROPOUT_RATE, num_layers=I_TRANSFORMER_LAYERS):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([self._make_g_block(dim, num_heads, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def _make_g_block(self, dim, num_heads, dropout):\n",
    "        \"\"\"Implements the g(.) block (Eq. 9, 10)\"\"\"\n",
    "        return nn.ModuleList([\n",
    "            nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True),\n",
    "            nn.LayerNorm(dim), \n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Dropout(dropout)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, Z):\n",
    "    \n",
    "        Z_start = Z\n",
    "        Z1 = self.pos_encoder(Z) \n",
    "        Z_out = Z1\n",
    "        for mha, norm1, ffn1, ffn2, norm2, drop1, drop2 in self.layers:\n",
    "            residual1 = Z_out\n",
    "            attn_out, _ = mha(Z_out, Z_out, Z_out)\n",
    "            Z_out = norm1(residual1 + drop1(attn_out)) \n",
    "            \n",
    "            residual2 = Z_out\n",
    "            ffw_out = ffn2(F.relu(ffn1(Z_out)))\n",
    "            Z_out = norm2(residual2 + drop2(ffw_out))\n",
    "    \n",
    "        Z4 = Z_out + Z_start\n",
    "        return Z4\n",
    "\n",
    "\n",
    "class DPTFBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the h(.) module: F-TE followed by T-TE (in series) (Fig. 2b)\n",
    "    Note: The paper implies shared weights across blocks (II.D)\n",
    "    \"\"\"\n",
    "    def __init__(self, N=FEATURE_DIM_N, H=N_HEADS):\n",
    "        super().__init__()\n",
    "        self.f_te = TransformerEncoder(N, H)\n",
    "        self.t_te = TransformerEncoder(N, H)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, F_p, T_p = x.shape \n",
    "        \n",
    "        x_freq = x.permute(0, 3, 2, 1).reshape(B * T_p, F_p, N)\n",
    "        x_freq = self.f_te(x_freq)\n",
    "        x_out = x_freq.reshape(B, T_p, F_p, N).permute(0, 3, 2, 1)\n",
    "\n",
    "        x_time = x_out.permute(0, 2, 3, 1).reshape(B * F_p, T_p, N)\n",
    "        x_time = self.t_te(x_time)\n",
    "        x_out = x_time.reshape(B, F_p, T_p, N).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    \"\"\"The Feature Transformer block (Fig. 2a)\"\"\"\n",
    "    def __init__(self, N=FEATURE_DIM_N, num_stacks=J_DPTF_STACKS):\n",
    "        super().__init__()\n",
    "        self.transformer_blocks = nn.ModuleList([DPTFBlock(N) for _ in range(num_stacks)])\n",
    "        \n",
    "        self.conv_tanh = nn.Conv2d(N, N, kernel_size=1)\n",
    "        self.conv_sigmoid = nn.Conv2d(N, N, kernel_size=1)\n",
    "        self.conv_final = nn.Conv2d(N, N, kernel_size=1)\n",
    "        self.norm_final = PermuteLayerNorm(N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Xt1 = x\n",
    "        for block in self.transformer_blocks:\n",
    "            Xt1 = block(Xt1)\n",
    "            \n",
    "        tanh_out = torch.tanh(self.conv_tanh(Xt1))\n",
    "        sigmoid_out = torch.sigmoid(self.conv_sigmoid(Xt1))\n",
    "        Xt2 = tanh_out * sigmoid_out\n",
    "        \n",
    "        Xt = self.norm_final(self.conv_final(Xt2))\n",
    "        return Xt\n",
    "\n",
    "\n",
    "class Separator(nn.Module):\n",
    "    def __init__(self, N=FEATURE_DIM_N, K=KERNEL_SIZE, S=STRIDES):\n",
    "        super().__init__()\n",
    "        self.conv_up = nn.ConvTranspose2d(N, N, kernel_size=K, stride=S, output_padding=1)\n",
    "        self.norm1 = PermuteLayerNorm(N)\n",
    "    \n",
    "        self.conv_final = nn.Conv2d(N, 4, kernel_size=1) \n",
    "\n",
    "    def forward(self, x, F_padded, T):\n",
    "       \n",
    "        \n",
    "        x = self.conv_up(x)\n",
    "        \n",
    "        x = F.interpolate(x, size=(F_padded, T), mode='nearest')\n",
    "        \n",
    "        Xs1 = F.relu(self.norm1(x))\n",
    "        \n",
    "        Xs = self.conv_final(Xs1)\n",
    "        return Xs\n",
    "\n",
    "\n",
    "class DPFTSeparator(nn.Module):\n",
    "    def __init__(self, num_transformer_stacks=J_DPTF_STACKS):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.feature_transformer = FeatureTransformer(num_stacks=num_transformer_stacks)\n",
    "        self.separator = Separator()\n",
    "        \n",
    "    def forward(self, xm):\n",
    "        Xm2_4d, Xm_original_batched = preprocess_signal(xm) \n",
    "        \n",
    "        B, C, F_padded, T = Xm2_4d.shape\n",
    "        Xe = self.feature_extractor(Xm2_4d)\n",
    "        Xt = self.feature_transformer(Xe)\n",
    "        Xs = self.separator(Xt, F_padded, T) \n",
    "        Xo1_separated, Xo2_separated = post_processing_block(Xs, Xm_original_batched)\n",
    "        xo1 = compute_istft(Xo1_separated)\n",
    "        xo2 = compute_istft(Xo2_separated)\n",
    "        \n",
    "        return xo1, xo2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
